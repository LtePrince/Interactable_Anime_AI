"""
è®­ç»ƒè„šæœ¬ - ç”¨äºè®­ç»ƒå¯äº¤äº’åŠ¨ç”»AIæ¨¡å‹
è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºå¦‚ä½•è®­ç»ƒConvLSTM U-Netæ¨¡å‹çš„ç¤ºä¾‹
"""

import numpy as np
import time
from typing import List, Dict, Tuple
import pickle
import os
from data_processor import DataProcessor, DataGenerator

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, frame_size=(5, 5)):
        self.frame_size = frame_size
        self.data_processor = DataProcessor(frame_size)
        self.data_generator = DataGenerator(frame_size)
        
        # è®­ç»ƒå‚æ•°
        self.learning_rate = 0.001
        self.batch_size = 32
        self.num_epochs = 100
        
        # æŸå¤±å†å²
        self.loss_history = []
        self.accuracy_history = []
        
    def generate_training_data(self, num_sequences: int = 1000, 
                             sequence_length: int = 10) -> List[Dict]:
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        print(f"ğŸ¯ ç”Ÿæˆè®­ç»ƒæ•°æ®...")
        print(f"   åºåˆ—æ•°é‡: {num_sequences}")
        print(f"   åºåˆ—é•¿åº¦: {sequence_length}")
        
        all_samples = []
        
        for i in range(num_sequences):
            if i % 100 == 0:
                print(f"   è¿›åº¦: {i}/{num_sequences}")
            
            sequence = self.data_generator.generate_synthetic_sequence(sequence_length)
            all_samples.extend(sequence)
        
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(all_samples)}")
        return all_samples
    
    def split_data(self, samples: List[Dict], 
                   train_ratio: float = 0.8) -> Tuple[List[Dict], List[Dict]]:
        """åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
        np.random.shuffle(samples)
        split_idx = int(len(samples) * train_ratio)
        
        train_samples = samples[:split_idx]
        val_samples = samples[split_idx:]
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²:")
        print(f"   è®­ç»ƒé›†: {len(train_samples)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_samples)} æ ·æœ¬")
        
        return train_samples, val_samples
    
    def compute_loss(self, predictions: np.ndarray, targets: np.ndarray) -> float:
        """è®¡ç®—æŸå¤±ï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰"""
        # é¿å…log(0)
        epsilon = 1e-7
        predictions = np.clip(predictions, epsilon, 1 - epsilon)
        
        # äºŒå…ƒäº¤å‰ç†µæŸå¤±
        loss = -np.mean(
            targets * np.log(predictions) + 
            (1 - targets) * np.log(1 - predictions)
        )
        
        return loss
    
    def compute_accuracy(self, predictions: np.ndarray, targets: np.ndarray) -> float:
        """è®¡ç®—å‡†ç¡®ç‡"""
        pred_binary = (predictions > 0.5).astype(np.float32)
        accuracy = np.mean(pred_binary == targets)
        return accuracy
    
    def simple_forward_pass(self, batch_data: Dict) -> np.ndarray:
        """ç®€åŒ–çš„å‰å‘ä¼ æ’­ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        frames = batch_data['frames']
        mouse = batch_data['mouse']
        emotion = batch_data['emotion']
        attention = batch_data['attention']
        
        batch_size = frames.shape[0]
        predictions = np.zeros((batch_size, *self.frame_size, 1), dtype=np.float32)
        
        for i in range(batch_size):
            # ç®€åŒ–çš„ç‰¹å¾èåˆ
            frame = frames[i, :, :, 0]
            mouse_features = mouse[i]
            emotion_features = emotion[i]
            attention_map = attention[i]
            
            # æå–å…³é”®ç‰¹å¾
            mouse_x = mouse_features[0] * (self.frame_size[1] - 1)
            mouse_y = mouse_features[1] * (self.frame_size[0] - 1)
            mouse_click = mouse_features[2]
            
            happiness = emotion_features[0]
            arousal = emotion_features[1]
            engagement = emotion_features[2]
            
            # åˆå§‹åŒ–é¢„æµ‹ä¸ºå½“å‰å¸§çš„å»¶ç»­
            pred = frame * 0.6  # å¸§å»¶ç»­æ€§
            
            # æ·»åŠ é¼ æ ‡å½±å“
            if mouse_click > 0.5:
                mouse_x_int = int(np.clip(mouse_x, 0, 4))
                mouse_y_int = int(np.clip(mouse_y, 0, 4))
                
                # åœ¨é¼ æ ‡ä½ç½®åŠå‘¨å›´æ·»åŠ æ¿€æ´»
                for dy in range(-1, 2):
                    for dx in range(-1, 2):
                        ny, nx = mouse_y_int + dy, mouse_x_int + dx
                        if 0 <= ny < 5 and 0 <= nx < 5:
                            distance = abs(dy) + abs(dx)
                            influence = 0.8 * (1.0 - distance * 0.3)
                            pred[ny, nx] = min(1.0, pred[ny, nx] + influence)
            
            # æ·»åŠ æƒ…ç»ªå½±å“
            emotion_base = happiness * 0.2  # å‡å°‘æƒ…ç»ªåŸºç¡€å½±å“
            pred += emotion_base
            
            # æ·»åŠ æ¿€æ´»åº¦éšæœºæ€§
            if arousal > 0.6:
                noise_mask = np.random.random(self.frame_size) < (arousal * 0.1)
                pred[noise_mask] += 0.3
            
            # åº”ç”¨æ³¨æ„åŠ›å›¾
            pred = pred * (0.5 + 0.5 * attention_map)
            
            # å½’ä¸€åŒ–å’Œé˜ˆå€¼å¤„ç†
            pred = np.clip(pred, 0, 1)
            
            # åŠ¨æ€é˜ˆå€¼
            threshold = 0.4 - engagement * 0.1
            threshold = max(0.2, min(0.6, threshold))
            
            pred = (pred > threshold).astype(np.float32)
            
            # ç¡®ä¿ä¸æ˜¯å…¨é»‘
            if np.sum(pred) == 0:
                # éšæœºæ¿€æ´»ä¸€ä¸ªç‚¹
                rand_y, rand_x = np.random.randint(0, 5, 2)
                pred[rand_y, rand_x] = 1.0
            
            predictions[i, :, :, 0] = pred
        
        return predictions
    
    def train_epoch(self, train_samples: List[Dict]) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        np.random.shuffle(train_samples)
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        for i in range(0, len(train_samples), self.batch_size):
            batch_samples = train_samples[i:i + self.batch_size]
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            batch_data = self.data_processor.batch_process(batch_samples)
            
            # å‰å‘ä¼ æ’­
            predictions = self.simple_forward_pass(batch_data)
            targets = batch_data['targets']
            
            # è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡
            loss = self.compute_loss(predictions, targets)
            accuracy = self.compute_accuracy(predictions, targets)
            
            total_loss += loss
            total_accuracy += accuracy
            num_batches += 1
        
        avg_loss = total_loss / num_batches
        avg_accuracy = total_accuracy / num_batches
        
        return avg_loss, avg_accuracy
    
    def validate(self, val_samples: List[Dict]) -> Tuple[float, float]:
        """éªŒè¯æ¨¡å‹"""
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        for i in range(0, len(val_samples), self.batch_size):
            batch_samples = val_samples[i:i + self.batch_size]
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            batch_data = self.data_processor.batch_process(batch_samples)
            
            # å‰å‘ä¼ æ’­
            predictions = self.simple_forward_pass(batch_data)
            targets = batch_data['targets']
            
            # è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡
            loss = self.compute_loss(predictions, targets)
            accuracy = self.compute_accuracy(predictions, targets)
            
            total_loss += loss
            total_accuracy += accuracy
            num_batches += 1
        
        avg_loss = total_loss / num_batches
        avg_accuracy = total_accuracy / num_batches
        
        return avg_loss, avg_accuracy
    
    def train(self, num_sequences: int = 1000, save_path: str = "model_checkpoint.pkl"):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒConvLSTMå¯äº¤äº’åŠ¨ç”»AIæ¨¡å‹")
        print("=" * 60)
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        samples = self.generate_training_data(num_sequences)
        train_samples, val_samples = self.split_data(samples)
        
        print(f"\nğŸ“ˆ è®­ç»ƒå‚æ•°:")
        print(f"   å­¦ä¹ ç‡: {self.learning_rate}")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"   è®­ç»ƒè½®æ•°: {self.num_epochs}")
        print(f"   å¸§å°ºå¯¸: {self.frame_size}")
        
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
        
        best_val_loss = float('inf')
        
        for epoch in range(self.num_epochs):
            start_time = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_samples)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_samples)
            
            # è®°å½•å†å²
            self.loss_history.append((train_loss, val_loss))
            self.accuracy_history.append((train_acc, val_acc))
            
            epoch_time = time.time() - start_time
            
            # æ‰“å°è¿›åº¦
            if epoch % 10 == 0 or epoch == self.num_epochs - 1:
                print(f"Epoch {epoch+1:3d}/{self.num_epochs}: "
                      f"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, "
                      f"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, "
                      f"time={epoch_time:.2f}s")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self.save_model(save_path)
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"   æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        
        return self.loss_history, self.accuracy_history
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œåªä¿å­˜è®­ç»ƒå†å²ï¼‰"""
        model_data = {
            'loss_history': self.loss_history,
            'accuracy_history': self.accuracy_history,
            'frame_size': self.frame_size,
            'training_params': {
                'learning_rate': self.learning_rate,
                'batch_size': self.batch_size,
                'num_epochs': self.num_epochs
            }
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.loss_history = model_data['loss_history']
        self.accuracy_history = model_data['accuracy_history']
        self.frame_size = model_data['frame_size']
        
        if 'training_params' in model_data:
            params = model_data['training_params']
            self.learning_rate = params['learning_rate']
            self.batch_size = params['batch_size']
            self.num_epochs = params['num_epochs']
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²ï¼ˆæ–‡æœ¬ç‰ˆæœ¬ï¼‰"""
        if not self.loss_history:
            print("æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®")
            return
        
        print("\nğŸ“Š è®­ç»ƒå†å²:")
        print("-" * 80)
        print("Epoch  Train Loss  Val Loss    Train Acc   Val Acc")
        print("-" * 80)
        
        step = max(1, len(self.loss_history) // 20)  # æœ€å¤šæ˜¾ç¤º20è¡Œ
        
        for i in range(0, len(self.loss_history), step):
            train_loss, val_loss = self.loss_history[i]
            train_acc, val_acc = self.accuracy_history[i]
            
            print(f"{i+1:5d}  {train_loss:10.4f}  {val_loss:9.4f}  "
                  f"{train_acc:10.4f}  {val_acc:8.4f}")
        
        print("-" * 80)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        final_train_loss, final_val_loss = self.loss_history[-1]
        final_train_acc, final_val_acc = self.accuracy_history[-1]
        
        print(f"æœ€ç»ˆç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {final_train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {final_val_loss:.4f}")
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {final_val_acc:.4f}")

def run_training_demo():
    """è¿è¡Œè®­ç»ƒæ¼”ç¤º"""
    print("ğŸ“ ConvLSTMå¯äº¤äº’åŠ¨ç”»AI è®­ç»ƒæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ModelTrainer(frame_size=(5, 5))
    
    # è¿è¡Œè®­ç»ƒï¼ˆä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œæ¼”ç¤ºï¼‰
    print("ğŸ“ æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è®­ç»ƒæ¼”ç¤º")
    print("   åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨éœ€è¦ï¼š")
    print("   1. ä½¿ç”¨TensorFlow/PyTorchå®ç°å®Œæ•´çš„ConvLSTM U-Net")
    print("   2. æ”¶é›†çœŸå®çš„ç”¨æˆ·äº¤äº’æ•°æ®")
    print("   3. ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ")
    print("   4. å®ç°æ›´å¤æ‚çš„æ•°æ®å¢å¼º")
    print()
    
    # å¼€å§‹è®­ç»ƒ
    loss_history, accuracy_history = trainer.train(
        num_sequences=100,  # å°æ•°æ®é›†ç”¨äºæ¼”ç¤º
        save_path="demo_model.pkl"
    )
    
    # æ˜¾ç¤ºè®­ç»ƒç»“æœ
    trainer.plot_training_history()
    
    print(f"\nğŸ’¾ æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜")
    print(f"ğŸ“ æ•°æ®å¤„ç†é…ç½®å¯ä¿å­˜ä¸ºJSONæ–‡ä»¶")

def analyze_data_patterns():
    """åˆ†ææ•°æ®æ¨¡å¼"""
    print("\nğŸ” æ•°æ®æ¨¡å¼åˆ†æ:")
    print("-" * 40)
    
    # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
    generator = DataGenerator(frame_size=(5, 5))
    processor = DataProcessor(frame_size=(5, 5))
    
    # ç”Ÿæˆä¸€äº›æ ·æœ¬æ•°æ®
    samples = generator.generate_synthetic_sequence(20)
    
    # åˆ†æé¼ æ ‡äº‹ä»¶åˆ†å¸ƒ
    mouse_clicks = sum(1 for s in samples if s['mouse_event']['click'])
    print(f"é¼ æ ‡ç‚¹å‡»é¢‘ç‡: {mouse_clicks}/{len(samples)} ({mouse_clicks/len(samples)*100:.1f}%)")
    
    # åˆ†ææƒ…ç»ªå˜åŒ–
    happiness_values = [s['emotion_state']['happiness'] for s in samples]
    avg_happiness = np.mean(happiness_values)
    print(f"å¹³å‡å¿«ä¹åº¦: {avg_happiness:.3f}")
    
    # åˆ†æå¸§å˜åŒ–
    frame_changes = []
    for i in range(1, len(samples)):
        prev_frame = samples[i-1]['current_frame']
        curr_frame = samples[i]['current_frame']
        change = np.sum(np.abs(curr_frame - prev_frame))
        frame_changes.append(change)
    
    avg_change = np.mean(frame_changes)
    print(f"å¹³å‡å¸§å˜åŒ–: {avg_change:.3f}")
    
    # æ‰¹å¤„ç†æ¼”ç¤º
    batch_data = processor.batch_process(samples[:8])
    print(f"\næ‰¹å¤„ç†æ•°æ®ç»´åº¦:")
    for key, value in batch_data.items():
        print(f"  {key}: {value.shape}")

if __name__ == "__main__":
    # è¿è¡Œè®­ç»ƒæ¼”ç¤º
    run_training_demo()
    
    # åˆ†ææ•°æ®æ¨¡å¼
    analyze_data_patterns()
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ’¡ ä¸‹ä¸€æ­¥å¯ä»¥ï¼š")
    print(f"   1. è¿è¡Œ SimpleConvLSTM_AI.py æµ‹è¯•äº¤äº’")
    print(f"   2. ä¿®æ”¹æ•°æ®ç”Ÿæˆè§„åˆ™é€‚åº”æ‚¨çš„éœ€æ±‚")
    print(f"   3. å®ç°å®Œæ•´çš„æ·±åº¦å­¦ä¹ æ¨¡å‹")
    print(f"   4. æ”¶é›†çœŸå®ç”¨æˆ·æ•°æ®è¿›è¡Œè®­ç»ƒ")
