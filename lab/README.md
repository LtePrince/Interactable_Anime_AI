# ConvLSTM 可交互动画AI系统

这是一个基于ConvLSTM U-Net架构的可交互动画AI系统，能够根据鼠标事件、情绪状态和当前动画帧生成下一帧的5x5点阵动画。

## 🎯 系统概述

### 输入
1. **鼠标事件** (5维向量)：
   - 鼠标X坐标（归一化到0-1）
   - 鼠标Y坐标（归一化到0-1）
   - 点击状态（0或1）
   - 移动事件（0或1）
   - 点击事件（0或1）

2. **情绪状态** (3维向量)：
   - 快乐度 (happiness: 0-1)
   - 激活度 (arousal: 0-1)
   - 参与度 (engagement: 0-1)

3. **当前动画帧** (5x5矩阵)：
   - 二值化的点阵图像

### 输出
- **下一帧动画** (5x5矩阵)：生成的新动画帧

## 📁 文件结构

```
lab/
├── SimpleConvLSTM_AI.py     # 主要的可交互AI系统（简化版）
├── ConvLSTM_Interactive_AI.py  # 完整的TensorFlow实现（需要安装TensorFlow）
├── data_processor.py       # 数据预处理模块
├── train_model.py         # 训练脚本和演示
├── README.md             # 本文件
└── ASCII_AI_simple.py    # 原始的ASCII AI（参考）
```

## 🚀 快速开始

### 1. 运行可交互AI
```bash
cd lab
python SimpleConvLSTM_AI.py
```

### 2. 运行训练演示
```bash
python train_model.py
```

### 3. 测试数据处理
```bash
python data_processor.py
```

## 🎮 操作指南

运行`SimpleConvLSTM_AI.py`后，可以使用以下命令：

- `h` - 增加快乐度
- `s` - 减少快乐度
- `e` - 激活兴奋模式
- `c` - 激活冷静模式
- `mouse:x,y,click` - 鼠标操作（例：`mouse:2,3,true`）
- `demo` - 运行演示模式
- `r` - 重置状态
- `q` - 退出

## 🧠 模型架构

### 简化版本（SimpleConvLSTM_AI.py）
```
输入融合 → 简化ConvLSTM → 特征提取 → 5x5输出
```

### 完整版本（ConvLSTM_Interactive_AI.py）
```
当前帧(5x5x1) ─┐
               ├─→ 特征融合 ─→ 上采样 ─→ ConvLSTM ─→ 下采样 ─→ 输出(5x5x1)
鼠标事件(5) ────┤
               │
情绪状态(3) ─────┘
```

### 详细架构
1. **输入处理层**：
   - 鼠标和情绪特征通过全连接层处理
   - 当前帧与特征图合并

2. **上采样阶段**：
   - 将5x5输入上采样到30x30
   - 使用卷积层提取特征

3. **ConvLSTM层**：
   - 处理时序信息
   - 保持空间结构

4. **下采样阶段**：
   - 将特征图下采样回5x5
   - 生成最终输出

## 🔧 技术细节

### 数据预处理（data_processor.py）
- **鼠标事件编码**：坐标归一化 + 事件类型编码
- **情绪状态编码**：多维情绪特征 + 派生特征
- **帧预处理**：归一化 + 通道扩展
- **空间注意力**：基于鼠标位置的注意力图
- **时序特征**：帧间差分和运动检测

### 训练流程（train_model.py）
1. **数据生成**：合成交互序列
2. **批处理**：高效的数据加载
3. **训练循环**：损失计算和参数更新
4. **验证**：模型性能评估
5. **保存**：模型检查点保存

## 📊 特征说明

### 鼠标事件特征
```python
[norm_x, norm_y, click, is_move, is_click, is_drag]
```

### 情绪状态特征
```python
[valence, energy, attention, excitement, calmness]
```

### 空间注意力图
基于鼠标位置的高斯分布注意力：
```python
attention = exp(-distance / radius)
```

## 🎨 交互效果

### 情绪影响
- **高快乐度**：使用开心表情基础模式
- **低快乐度**：使用难过表情基础模式
- **高激活度**：添加随机闪烁效果
- **高参与度**：降低激活阈值，更容易响应

### 鼠标交互
- **点击效果**：在点击位置及周围创建激活
- **移动效果**：增加参与度
- **空间影响**：基于距离的衰减效果

### 时序连续性
- **帧间连续性**：保持动画的流畅性
- **状态记忆**：ConvLSTM保持历史信息
- **自然衰减**：情绪和激活的自然衰减

## 🛠️ 扩展建议

### 1. 完整深度学习实现
- 使用TensorFlow/PyTorch实现完整模型
- 添加更多层和复杂的架构
- 使用GPU加速训练

### 2. 数据增强
- 添加更多合成数据规则
- 实现真实用户数据收集
- 使用生成对抗网络(GAN)

### 3. 高级特征
- 多帧历史输入
- 更复杂的情绪模型
- 用户个性化适应

### 4. 应用扩展
- 更大的输出尺寸（如16x16）
- 彩色输出
- 3D动画支持
- 声音交互

## 📈 性能指标

### 训练指标
- **损失函数**：二元交叉熵
- **准确率**：像素级准确率
- **收敛速度**：训练轮数
- **泛化能力**：验证集表现

### 交互质量
- **响应延迟**：用户输入到视觉反馈的时间
- **视觉连贯性**：动画的流畅性
- **情绪表达准确性**：情绪状态的视觉表现

## 🐛 已知限制

1. **简化模型**：当前实现为演示版本，不是完整的深度学习模型
2. **固定尺寸**：只支持5x5输出
3. **有限情绪**：情绪模型相对简单
4. **合成数据**：训练数据为程序生成，非真实用户数据

## 🔮 未来改进

1. **模型复杂度**：实现完整的ConvLSTM U-Net
2. **多模态输入**：添加声音、文本等输入
3. **个性化**：用户特定的模型适应
4. **实时性**：优化推理速度
5. **云端部署**：支持在线服务

## 📚 参考资料

- ConvLSTM论文：Convolutional LSTM Network
- U-Net架构：U-Net: Convolutional Networks for Biomedical Image Segmentation
- 情绪计算：Affective Computing相关研究
- 人机交互：HCI (Human-Computer Interaction) 理论

## 🤝 贡献

欢迎提交问题和改进建议！这个项目是一个研究和学习的平台，适合：
- 深度学习研究者
- 人机交互开发者
- 游戏AI开发者
- 情绪计算研究者

---

*这个项目展示了如何将ConvLSTM U-Net应用于可交互动画生成，为更复杂的AI交互系统提供了基础框架。*
